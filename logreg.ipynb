{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.statsmodels\n",
    "from mlflow import MlflowClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = pd.read_parquet(\"donnees/preproces.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prep['is_claim']\n",
    "x = prep.drop('is_claim',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2, random_state = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model\n",
    "model1 = LogisticRegression(max_iter=1000,random_state=0).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model1.predict(X_test)\n",
    "confmtrx = np.array(confusion_matrix(y_test, preds))\n",
    "confusion = pd.DataFrame(confmtrx, index=['approved', 'not_approved'],\n",
    "columns=['predicted_approved', 'predicted_not_approved'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy and log the metric using MLflow\n",
    "accuracy = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model=sm.Logit(y_train,sm.add_constant(X_train))\n",
    "logit_model\n",
    "result=logit_model.fit()\n",
    "stats1=result.summary()\n",
    "print(stats1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name1 = \"oop6\"\n",
    "mlflow.set_experiment(experiment_name1)\n",
    "mlflow.statsmodels.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "        score = model1.score(X_train, y_train)\n",
    "        print(f\"Score: {score}\")\n",
    "        mlflow.log_metric(\"score\", score)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        predictions = model1.predict(X_train)\n",
    "        signature = infer_signature(X_train, predictions)\n",
    "        mlflow.sklearn.log_model(model1, \"model\", signature=signature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the experiment name or ID where the run was logged\n",
    "\n",
    "\n",
    "# Search for the run and retrieve the logged metrics\n",
    "runs = mlflow.search_runs(experiment_name=experiment_name1)\n",
    "latest_run = runs.iloc[0]  # Assumes you want the latest run\n",
    "accuracy_metric = latest_run['accuracy']\n",
    "\n",
    "print(f\"Accuracy metric: {accuracy_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_logreg_model = mlflow.pyfunc.load_model(sklearn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_logreg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model1, open(\"logi_regre.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a61",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
